{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7764ea01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, ShuffleSplit\n",
    "import numpy as np\n",
    "from os import makedirs\n",
    "from os.path import join\n",
    "import xgboost as xgb\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, r2_score\n",
    "import sklearn.model_selection\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from functools import partial\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de90a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('/external/rprshnas01/tigrlab/scratch/bng/cartbind/data/ukb_FIS_all_no_outliers.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7cc432",
   "metadata": {},
   "source": [
    "### Using FC to predict Fluid Intelligence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded71b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns renamed successfully.\n"
     ]
    }
   ],
   "source": [
    "# rename columns\n",
    "datafield_code = ['31-0.0', '21003-2.0', '54-2.0']\n",
    "\n",
    "datafield_name = ['sex', 'age', 'assessment centre']\n",
    "\n",
    "if len(datafield_code) == len(datafield_name):\n",
    "    rename_dict = dict(zip(datafield_code, datafield_name))\n",
    "    df = df.rename(columns=rename_dict)\n",
    "    print(\"Columns renamed successfully.\")\n",
    "else:\n",
    "    print(\"Error: The number of datafield codes does not match the number of datafield names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3fa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_variables = ['age',\n",
    "        \n",
    "        'IC1IC2', 'IC1IC3', 'IC1IC4', 'IC1IC5', 'IC1IC6', 'IC1IC7', 'IC1IC8', \n",
    "        'IC1IC9', 'IC1IC10', 'IC1IC11', 'IC1IC12', 'IC1IC13', 'IC1IC14', \n",
    "        'IC1IC15', 'IC1IC16', 'IC1IC17', 'IC1IC18', 'IC1IC19', 'IC1IC20', \n",
    "        'IC1IC21', 'IC2IC3', 'IC2IC4', 'IC2IC5', 'IC2IC6', 'IC2IC7', 'IC2IC8', \n",
    "        'IC2IC9', 'IC2IC10', 'IC2IC11', 'IC2IC12', 'IC2IC13', 'IC2IC14', 'IC2IC15', \n",
    "        'IC2IC16', 'IC2IC17', 'IC2IC18', 'IC2IC19', 'IC2IC20', 'IC2IC21', 'IC3IC4', \n",
    "        'IC3IC5', 'IC3IC6', 'IC3IC7', 'IC3IC8', 'IC3IC9', 'IC3IC10', 'IC3IC11', \n",
    "        'IC3IC12', 'IC3IC13', 'IC3IC14', 'IC3IC15', 'IC3IC16', 'IC3IC17', 'IC3IC18', \n",
    "        'IC3IC19', 'IC3IC20', 'IC3IC21', 'IC4IC5', 'IC4IC6', 'IC4IC7', 'IC4IC8', \n",
    "        'IC4IC9', 'IC4IC10', 'IC4IC11', 'IC4IC12', 'IC4IC13', 'IC4IC14', 'IC4IC15', \n",
    "        'IC4IC16', 'IC4IC17', 'IC4IC18', 'IC4IC19', 'IC4IC20', 'IC4IC21', 'IC5IC6', \n",
    "        'IC5IC7', 'IC5IC8', 'IC5IC9', 'IC5IC10', 'IC5IC11', 'IC5IC12', 'IC5IC13', \n",
    "        'IC5IC14', 'IC5IC15', 'IC5IC16', 'IC5IC17', 'IC5IC18', 'IC5IC19', 'IC5IC20', \n",
    "        'IC5IC21', 'IC6IC7', 'IC6IC8', 'IC6IC9', 'IC6IC10', 'IC6IC11', 'IC6IC12', \n",
    "        'IC6IC13', 'IC6IC14', 'IC6IC15', 'IC6IC16', 'IC6IC17', 'IC6IC18', 'IC6IC19', \n",
    "        'IC6IC20', 'IC6IC21', 'IC7IC8', 'IC7IC9', 'IC7IC10', 'IC7IC11', 'IC7IC12', \n",
    "        'IC7IC13', 'IC7IC14', 'IC7IC15', 'IC7IC16', 'IC7IC17', 'IC7IC18', 'IC7IC19', \n",
    "        'IC7IC20', 'IC7IC21', 'IC8IC9', 'IC8IC10', 'IC8IC11', 'IC8IC12', 'IC8IC13', \n",
    "        'IC8IC14', 'IC8IC15', 'IC8IC16', 'IC8IC17', 'IC8IC18', 'IC8IC19', 'IC8IC20', \n",
    "        'IC8IC21', 'IC9IC10', 'IC9IC11', 'IC9IC12', 'IC9IC13', 'IC9IC14', 'IC9IC15', \n",
    "        'IC9IC16', 'IC9IC17', 'IC9IC18', 'IC9IC19', 'IC9IC20', 'IC9IC21', 'IC10IC11', \n",
    "        'IC10IC12', 'IC10IC13', 'IC10IC14', 'IC10IC15', 'IC10IC16', 'IC10IC17', 'IC10IC18', \n",
    "        'IC10IC19', 'IC10IC20', 'IC10IC21', 'IC11IC12', 'IC11IC13', 'IC11IC14', 'IC11IC15', \n",
    "        'IC11IC16', 'IC11IC17', 'IC11IC18', 'IC11IC19', 'IC11IC20', 'IC11IC21', 'IC12IC13', \n",
    "        'IC12IC14', 'IC12IC15', 'IC12IC16', 'IC12IC17', 'IC12IC18', 'IC12IC19', 'IC12IC20', \n",
    "        'IC12IC21', 'IC13IC14', 'IC13IC15', 'IC13IC16', 'IC13IC17', 'IC13IC18', 'IC13IC19', \n",
    "        'IC13IC20', 'IC13IC21', 'IC14IC15', 'IC14IC16', 'IC14IC17', 'IC14IC18', 'IC14IC19', \n",
    "        'IC14IC20', 'IC14IC21', 'IC15IC16', 'IC15IC17', 'IC15IC18', 'IC15IC19', 'IC15IC20', \n",
    "        'IC15IC21', 'IC16IC17', 'IC16IC18', 'IC16IC19', 'IC16IC20', 'IC16IC21', 'IC17IC18', \n",
    "        'IC17IC19', 'IC17IC20', 'IC17IC21', 'IC18IC19', 'IC18IC20', 'IC18IC21', 'IC19IC20', \n",
    "        'IC19IC21', 'IC20IC21']\n",
    "\n",
    "categorical_variables = ['assessment centre']\n",
    "\n",
    "binary_variables = ['sex']\n",
    "\n",
    "output_variables = ['20016-2.0']\n",
    "\n",
    "input_variables = list(set(numerical_variables + categorical_variables + binary_variables) - set(output_variables))\n",
    "df[categorical_variables] = df[categorical_variables].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3794df",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 1000, 20),\n",
    "    'eta': hp.quniform('eta', 0.025, 0.8, 0.025),\n",
    "    # A problem with max_depth casted to float instead of int with\n",
    "    # the hp.quniform method.\n",
    "    'max_depth': hp.choice('max_depth', np.arange(1, 10, dtype=int)),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "    'subsample': hp.quniform('subsample', 0.2, 1, 0.1),\n",
    "    'gamma': hp.quniform('gamma', 0, 10, 0.2),\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "    'lambda': hp.loguniform('lambda', np.log(1e-4), np.log(1e4)),  # L2 regularization\n",
    "    'alpha': hp.loguniform('alpha', np.log(1e-4), np.log(1e4)),    # L1 regularization\n",
    "    'eval_metric': 'auc',\n",
    "    'objective': 'binary:logistic',\n",
    "    # Increase this number if you have more cores. Otherwise, remove it and it will default\n",
    "    # to the maximum number.\n",
    "    # 'nthread': 12,\n",
    "    'booster': 'gbtree',\n",
    "    'tree_method': 'hist',\n",
    "    'seed': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b3696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(params, data):\n",
    "    data_x = data[0]\n",
    "    data_y = data[1]\n",
    "    train_features_instance = data_x[0]\n",
    "    valid_features_instance = data_x[1]\n",
    "    y_train_instance = data_y[0]\n",
    "    y_valid_instance = data_y[1]\n",
    "\n",
    "    print(\"Training with params: \")\n",
    "    print(params)\n",
    "    num_round = int(params['n_estimators'])\n",
    "    del params['n_estimators']\n",
    "    dtrain = xgb.DMatrix(train_features_instance, label=y_train_instance, enable_categorical=True)\n",
    "    dvalid = xgb.DMatrix(valid_features_instance, label=y_valid_instance, enable_categorical=True)\n",
    "    watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "    gbm_model = xgb.train(params, dtrain, num_round,\n",
    "                          evals=watchlist,\n",
    "                          verbose_eval=False)\n",
    "    predictions = gbm_model.predict(dvalid)\n",
    "    if y_train_instance.unique().shape[0] < 3:\n",
    "        score = roc_auc_score(y_valid_instance, predictions)\n",
    "    else:\n",
    "        score = r2_score(y_valid_instance, predictions)\n",
    "        # TODO: Add the importance for the selected features\n",
    "        print(\"\\tScore {0}\\n\\n\".format(score))\n",
    "    # The score function should return the loss (1-score)\n",
    "    # since the optimize function looks for the minimum\n",
    "    loss = 1 - score\n",
    "    return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def hyper_parameter_optimization(data_df, input_variables, output_variable, numeric_vars, \n",
    "                                 categorical_vars, binary_vars, space=space):\n",
    "    number_of_splits = 10\n",
    "    #     print(data_df.shape)\n",
    "    # Data loading\n",
    "    data_df = data_df.dropna(subset=[output_variable]).copy(deep=True)\n",
    "    X = data_df[input_variables]\n",
    "    y = data_df[output_variable]\n",
    "    #     print(np.unique(y))\n",
    "    if y.unique().shape[0] < 3:\n",
    "        split_object = StratifiedShuffleSplit(n_splits=number_of_splits,\n",
    "                               train_size=(number_of_splits - 1) / number_of_splits,\n",
    "                               test_size=1 / number_of_splits,\n",
    "                               random_state=42)\n",
    "    else:\n",
    "        split_object = ShuffleSplit(n_splits=number_of_splits,\n",
    "                                              train_size=(number_of_splits - 1) / number_of_splits,\n",
    "                                              test_size=1 / number_of_splits,\n",
    "                                              random_state=42)\n",
    "\n",
    "    blocks = np.arange(y.shape[0])\n",
    "    for splt_idx, (train_idx, test_idx) in enumerate(split_object.split(blocks, y)):\n",
    "        X_trainval = X.iloc[train_idx, :]\n",
    "        y_trainval = y.iloc[train_idx]\n",
    "        X_test = X.iloc[test_idx, :]\n",
    "        y_test = y.iloc[test_idx]\n",
    "        if y.unique().shape[0] < 3:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.125, random_state=42,\n",
    "                                                              stratify=y_trainval)\n",
    "        else:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.125, random_state=42)\n",
    "        X_values = (X_trainval.copy(deep=True), X_test.copy(deep=True))\n",
    "        y_values = (y_trainval.copy(deep=True), y_test.copy(deep=True))\n",
    "\n",
    "        if y_train.unique().shape[0] >= 3:\n",
    "            space['eval_metric'] = 'rmse'\n",
    "            space['objective'] = 'reg:squarederror'\n",
    "        else:\n",
    "            space['eval_metric'] = 'auc'\n",
    "            space['objective'] = 'binary:logistic'\n",
    "        normalizer = StandardScaler()\n",
    "        to_normalize = X_trainval[numeric_vars].values\n",
    "        X_train[numeric_vars] = normalizer.fit_transform(X_train[numeric_vars])\n",
    "        X_val[numeric_vars] = normalizer.transform(X_val[numeric_vars])\n",
    "        train_features_instance = X_train.copy(deep=True)\n",
    "        y_train_instance = y_train.copy(deep=True)\n",
    "        valid_features_instance = X_val.copy(deep=True)\n",
    "        y_valid_instance = y_val.copy(deep=True)\n",
    "\n",
    "        score_data = partial(score, data=((X_train, X_val), (y_train, y_val)))\n",
    "        best_hyperparams = fmin(fn=score_data,\n",
    "                                space=space,\n",
    "                                algo=tpe.suggest,\n",
    "                                max_evals=500)\n",
    "\n",
    "        columns = X_test.columns\n",
    "        # print(data_x, data_y, best_hyperparameters, column_names)\n",
    "        dir_name = f'/external/rprshnas01/tigrlab/scratch/bng/cartbind/data/hyperparameters/best_hyperparameters_FC_06-28/{output_variable}/split_{splt_idx}'\n",
    "        makedirs(dir_name, exist_ok=True)\n",
    "        column_names = np.array(list(columns))\n",
    "        np.savez(join(dir_name, 'train_test_data.npz'), x_train=X_trainval, y_train=y_trainval,\n",
    "                 x_test=X_test, y_test=y_test, column_names=column_names)\n",
    "        # json object getting serialised\n",
    "        best_hyperparameters_json = json.dumps(best_hyperparams, indent=4, cls=NpEncoder)\n",
    "        # Writing\n",
    "        with open(join(dir_name, 'best_hyperparameters.json'), \"w\") as outfile:\n",
    "            outfile.write(best_hyperparameters_json)\n",
    "\n",
    "    return X_values, y_values, best_hyperparams, columns\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07095f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization...\n",
      "Training with params:                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.8, 'eta': 0.30000000000000004, 'eval_metric': 'rmse', 'gamma': 1.0, 'max_depth': 4, 'min_child_weight': 2.0, 'n_estimators': 890.0, 'objective': 'reg:squarederror', 'seed': 42, 'subsample': 0.6000000000000001, 'tree_method': 'hist'}\n",
      "\tScore -0.25089672197770363                            \n",
      "\n",
      "\n",
      "Training with params:                                                            \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.7000000000000001, 'eta': 0.25, 'eval_metric': 'rmse', 'gamma': 0.75, 'max_depth': 7, 'min_child_weight': 4.0, 'n_estimators': 409.0, 'objective': 'reg:squarederror', 'seed': 42, 'subsample': 0.75, 'tree_method': 'hist'}\n",
      "\tScore -0.13353586941650986                                                      \n",
      "\n",
      "\n",
      "Training with params:                                                            \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.5, 'eta': 0.35000000000000003, 'eval_metric': 'rmse', 'gamma': 0.9500000000000001, 'max_depth': 12, 'min_child_weight': 2.0, 'n_estimators': 992.0, 'objective': 'reg:squarederror', 'seed': 42, 'subsample': 0.75, 'tree_method': 'hist'}\n",
      "\tScore -0.20861896256839008                                                      \n",
      "\n",
      "\n",
      "Training with params:                                                            \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.7000000000000001, 'eta': 0.17500000000000002, 'eval_metric': 'rmse', 'gamma': 0.9, 'max_depth': 7, 'min_child_weight': 5.0, 'n_estimators': 693.0, 'objective': 'reg:squarederror', 'seed': 42, 'subsample': 0.75, 'tree_method': 'hist'}\n",
      "  1%|          | 3/500 [00:06<17:45,  2.14s/trial, best loss: 1.1335358694165099]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting hyperparameter optimization...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m data_x, data_y, best_hyperparameters, column_names \u001b[38;5;241m=\u001b[39m \u001b[43mhyper_parameter_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mnumerical_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyperparameter optimization completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 82\u001b[0m, in \u001b[0;36mhyper_parameter_optimization\u001b[0;34m(data_df, input_variables, output_variable, numeric_vars, categorical_vars, binary_vars, space)\u001b[0m\n\u001b[1;32m     79\u001b[0m y_valid_instance \u001b[38;5;241m=\u001b[39m y_val\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     81\u001b[0m score_data \u001b[38;5;241m=\u001b[39m partial(score, data\u001b[38;5;241m=\u001b[39m((X_train, X_val), (y_train, y_val)))\n\u001b[0;32m---> 82\u001b[0m best_hyperparams \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m columns \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# print(data_x, data_y, best_hyperparameters, column_names)\u001b[39;00m\n",
      "File \u001b[0;32m~/MIND_models/venv/lib/python3.8/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/MIND_models/venv/lib/python3.8/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/MIND_models/venv/lib/python3.8/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/MIND_models/venv/lib/python3.8/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/MIND_models/venv/lib/python3.8/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mscore\u001b[0;34m(params, data)\u001b[0m\n\u001b[1;32m     14\u001b[0m dvalid \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(valid_features_instance, label\u001b[38;5;241m=\u001b[39my_valid_instance, enable_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m watchlist \u001b[38;5;241m=\u001b[39m [(dvalid, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m), (dtrain, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m---> 16\u001b[0m gbm_model \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_round\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwatchlist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m predictions \u001b[38;5;241m=\u001b[39m gbm_model\u001b[38;5;241m.\u001b[39mpredict(dvalid)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_train_instance\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[0;32m~/MIND_models/venv/lib/python3.8/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MIND_models/venv/lib/python3.8/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/MIND_models/venv/lib/python3.8/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_input = df\n",
    "input_vars = input_variables\n",
    "output_var = output_variables[0]\n",
    "numerical_vars = numerical_variables\n",
    "categorical_vars = categorical_variables\n",
    "binary_vars = binary_variables\n",
    "\n",
    "start = time.time()\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "\n",
    "data_x, data_y, best_hyperparameters, column_names = hyper_parameter_optimization(df_input, input_vars, output_var,\n",
    "                            numerical_vars, categorical_vars, binary_vars, space=space)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Hyperparameter optimization completed in {end - start} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
